{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PointNet aims to capture both local and global features from point cloud data. Local features pertain to the characteristics of individual points or small local neighborhoods, while global features represent the overall structure of the entire point cloud.\n",
    "\n",
    "PointNet employs shared MLPs to extract features from individual points independently. Each point is processed by the same MLP, ensuring weight sharing and translation invariance across points.\n",
    "\n",
    "After processing individual points through shared MLPs, PointNet utilizes max pooling to aggregate features across all points, producing a fixed-size global feature vector.\n",
    "This step effectively summarizes the local features extracted from the point cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import glob\n",
    "import trimesh\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/ModelNet10\\\\bathtub',\n",
       " './data/ModelNet10\\\\bed',\n",
       " './data/ModelNet10\\\\chair',\n",
       " './data/ModelNet10\\\\desk',\n",
       " './data/ModelNet10\\\\dresser',\n",
       " './data/ModelNet10\\\\monitor',\n",
       " './data/ModelNet10\\\\night_stand',\n",
       " './data/ModelNet10\\\\sofa',\n",
       " './data/ModelNet10\\\\table',\n",
       " './data/ModelNet10\\\\toilet']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dtataset\n",
    "root_dir = './data/ModelNet10/'\n",
    "folders = glob.glob(os.path.join(root_dir, \"*\"))\n",
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "def preprocess_data(num_points=2048):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    classes = {}\n",
    "\n",
    "    for i, folder in enumerate(folders):\n",
    "\n",
    "        print(\"processing class: \", os.path.basename(folder))\n",
    "\n",
    "        train_files = glob.glob(os.path.join(folder, \"train/*\"))\n",
    "        test_files = glob.glob(os.path.join(folder, \"test/*\"))\n",
    "\n",
    "        for f in train_files:\n",
    "            X_train.append(trimesh.load(f).sample(num_points))\n",
    "            y_train.append(i)\n",
    "\n",
    "        for f in test_files:\n",
    "            X_test.append(trimesh.load(f).sample(num_points))\n",
    "            y_test.append(i)\n",
    "        \n",
    "        classes[i] = folder.split(\"/\")[-1]\n",
    "\n",
    "    return (np.array(X_train), np.array(y_train), np.array(X_test), np.array(y_test), classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing class:  bathtub\n",
      "processing class:  bed\n",
      "processing class:  chair\n",
      "processing class:  desk\n",
      "processing class:  dresser\n",
      "processing class:  monitor\n",
      "processing class:  night_stand\n",
      "processing class:  sofa\n",
      "processing class:  table\n",
      "processing class:  toilet\n"
     ]
    }
   ],
   "source": [
    "num_points = 2048\n",
    "num_classes = len(folders) #10\n",
    "X_train, y_train, X_test, y_test, classes = preprocess_data(num_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(points, label):\n",
    "    points += tf.random.uniform(points.shape, -0.005, 0.005, dtype=\"float64\") #add randon noise\n",
    "    points = tf.random.shuffle(points)\n",
    "    return points, label\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(len(X_train)).map(augment).batch(32)\n",
    "test_dataset = test_dataset.shuffle(len(X_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PointNet Model\n",
    "def conv_layer(x, filters):\n",
    "    x = layers.Conv1D(filters, kernel_size=1, padding=\"valid\")(x)\n",
    "    x = layers.BatchNormalization(momentum=0.0)(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    return x\n",
    "\n",
    "def dense_layer(x, filters):\n",
    "    x = layers.Dense(filters)(x)\n",
    "    x = layers.BatchNormalization(momentum=0.0)(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrthogonalRegularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, num_features, l2reg=0.001):\n",
    "        self.num_features = num_features\n",
    "        self.l2reg = l2reg\n",
    "        self.eye = tf.eye(num_features)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = tf.reshape(x, (-1, self.num_features, self.num_features))\n",
    "        xxt = tf.tensordot(x, x, axes=(2, 2))\n",
    "        xxt = tf.reshape(xxt, (-1, self.num_features, self.num_features))\n",
    "        return tf.reduce_sum(self.l2reg * tf.square(xxt - self.eye))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tnet(inputs, num_features):\n",
    "\n",
    "    bias = keras.initializers.Constant(np.eye(num_features).flatten())\n",
    "    reg = OrthogonalRegularizer(num_features)\n",
    "\n",
    "    x = conv_layer(inputs, 32)\n",
    "    x = conv_layer(x, 64)\n",
    "    x = conv_layer(x, 512)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = dense_layer(x, 256)\n",
    "    x = dense_layer(x, 128)\n",
    "    x = layers.Dense(num_features * num_features, kernel_initializer=\"zeros\", bias_initializer=bias, activity_regularizer=reg)(x)\n",
    "    feat_T = layers.Reshape((num_features, num_features))(x)\n",
    "\n",
    "    return layers.Dot(axes=(2, 1))([inputs, feat_T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(num_points, 3))\n",
    "\n",
    "x = tnet(inputs, 3)\n",
    "x = conv_layer(x, 32)\n",
    "x = conv_layer(x, 32)\n",
    "x = tnet(x, 32)\n",
    "x = conv_layer(x, 32)\n",
    "x = conv_layer(x, 64)\n",
    "x = conv_layer(x, 512)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = dense_layer(x, 256)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = dense_layer(x, 128)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"pointnet\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile( loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=[\"sparse_categorical_accuracy\"])\n",
    "model.fit(train_dataset, epochs=50, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = test_dataset.take(1)\n",
    "\n",
    "points, labels = list(data)[0]\n",
    "points = points[:8, ...]\n",
    "labels = labels[:8, ...]\n",
    "\n",
    "# run test data through model\n",
    "preds = model.predict(points)\n",
    "preds = tf.math.argmax(preds, -1)\n",
    "\n",
    "points = points.numpy()\n",
    "\n",
    "# plot points with predicted class and label\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "for i in range(8):\n",
    "    ax = fig.add_subplot(2, 4, i + 1, projection=\"3d\")\n",
    "    ax.scatter(points[i, :, 0], points[i, :, 1], points[i, :, 2])\n",
    "    ax.set_title(\n",
    "        \"pred: {:}, label: {:}\".format(\n",
    "            classes[preds[i].numpy()], classes[labels.numpy()[i]]\n",
    "        )\n",
    "    )\n",
    "    ax.set_axis_off()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
